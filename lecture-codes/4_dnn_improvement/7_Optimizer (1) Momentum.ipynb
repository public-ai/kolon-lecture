{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Public AI</b></i>\n",
    "\n",
    "# Optimizer (1) Momentum\n",
    "\n",
    "### _Objective_\n",
    "\n",
    "\n",
    "* Momentum 알고리즘의 원리인 지수이동평균에 대해 배워보겠습니다. <br>\n",
    "* Gradient Descent 알고리즘의 개량 버전인 Momentum 알고리즘을 배워보겠습니다. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(30)\n",
    "if \"set_random_seed\" in dir(tf.random):\n",
    "    tf.random.set_random_seed(30)\n",
    "else:\n",
    "    tf.random.set_seed(30)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 1. 지수이동평균 \\]\n",
    "\n",
    "----\n",
    "\n",
    "* 지수 이동 평균이란, 시계열적 데이터에서 전체적인 추세를 확인하는 방법론입니다.\n",
    "* 증권가의 차트를 분석할 때 많이 등장하는 것으로, 변화가 극심한 차트 정보에서, 큰 추세의 변화를 알아내는 방법론입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 데이터) 2004.01~2018.05까지의 코스피 지수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = get_file(\"KOSPI_dataset.csv\",\n",
    "                 \"https://s3.ap-northeast-2.amazonaws.com/pai-datasets/alai-deeplearning/KOSPI_dataset.csv\")\n",
    "\n",
    "df = pd.read_csv(fpath,index_col=0)\n",
    "df.plot(y='DATA_VALUE',title=\"kospi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. 지수이동평균 (EMA, exponential Moving Average) 수식\n",
    "\n",
    "\n",
    "* 지수이동평균의 수식은 아래와 같습니다.<br>\n",
    "$\n",
    "s_{n+1} = (1-\\beta) x_n + \\beta s_n\\\\\n",
    "x_n: \\mbox{최신 정보} \\\\\n",
    "s_n: \\mbox{과거 종합 정보} \\\\\n",
    "\\beta : \\mbox{계수},0<\\beta<1\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 지수이동평균값($s_{n+1}$)의 재귀식을 풀어보면 아래와 같이 나옵니다.\n",
    "$\n",
    "s_{n+1} = (1-\\beta) ( x_n + \\beta x_{n-1}+\\beta^2x_{n-2}+ \\cdots + \\beta^{n}x_{0})\n",
    "$\n",
    "* 과거의 정보일수록 가중치가 점점 작아지는 특성을 가집니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. 지수이동 평균 계산하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) $\\beta$가 0.9일 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "ema = 0\n",
    "ema_list = []\n",
    "\n",
    "for x in df.DATA_VALUE:\n",
    "    ema = (1-beta)*x + beta*ema\n",
    "    ema_list.append(ema)\n",
    "\n",
    "df['beta=0.9'] = ema_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=['DATA_VALUE','beta=0.9'],title=\"kospi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) $\\beta$가 0.5일 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.5\n",
    "ema = 0\n",
    "ema_list = []\n",
    "\n",
    "for x in df.DATA_VALUE:\n",
    "    ema = (1-beta)*x + beta*ema\n",
    "    ema_list.append(ema)\n",
    "\n",
    "df['beta=0.5'] = ema_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=['DATA_VALUE','beta=0.9','beta=0.5'],title=\"kospi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta 값이 작아질수록 좀 더 변화에 민감하게 움직인다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 편향 보정 수식\n",
    "\n",
    "*  EMA 값을 구할 때, 초기 EMA 값이 너무 적게 계산됩니다.<br>\n",
    "* 이 문제를 해결하기 위해 기본적으로 편향 보정 가중치를 곱해주게 됩니다.<br>\n",
    "$\n",
    "s_{n+1} =\\frac{1}{1-\\beta^{n+1}}((1-\\beta) x_n + \\beta s_n)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)  $\\beta$가 0.9일 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "ema = 0\n",
    "ema_list = []\n",
    "\n",
    "for idx, x in enumerate(df.DATA_VALUE):\n",
    "    ema = (1-beta)*x + beta*ema\n",
    "    adjusted_ema = ema / (1-beta**(idx+1))\n",
    "    ema_list.append(adjusted_ema)\n",
    "\n",
    "df['beta=0.9'] = ema_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=['DATA_VALUE','beta=0.9'],title=\"kospi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) $\\beta$가 0.5일 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.5\n",
    "ema = 0\n",
    "ema_list = []\n",
    "\n",
    "for idx,x in enumerate(df.DATA_VALUE):\n",
    "    ema = (1-beta)*x + beta*ema\n",
    "    adjusted_ema = ema / (1-beta**(idx+1))\n",
    "    ema_list.append(adjusted_ema)\n",
    "\n",
    "df['beta=0.5'] = ema_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=['DATA_VALUE','beta=0.9','beta=0.5'],title=\"kospi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편향 보정을 통해, 초기에 값이 지나치게 작게 평가되는 문제를 방지하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 2. Momentum Optimizer 수식 \\]\n",
    "----\n",
    "\n",
    "* 모멘텀 알고리즘은 Gradient Descent 알고리즘에 지수이동평균 수식을 포함한 수식입니다.\n",
    "* 모멘텀은 Gradient Descent이 보다 빨리 수렴될 수 있도록 도와줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Gradient Descent Optimizer 수식\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial Loss}{\\partial W}\n",
    "$$\n",
    "\n",
    "원래 경사하강법의 수식은 위와 같습니다. 이 수식이 어떻게 변형되는지를 한번 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Momentum Optimizer 수식\n",
    "\n",
    "$$\n",
    "V := \\beta V +\\frac{\\partial L}{\\partial W} \\\\\n",
    "W := W - \\alpha V\n",
    "$$\n",
    "\n",
    "* 모멘텀 Optimizer의 수식은 지수이동평균 공식을 이용하되, 좀 더 간결하게 표기하였습니다.$(1-\\beta)$를 제거하였습니다.\n",
    "\n",
    "* 모멘텀은 물리에서 관성을 의미합니다. 관성의 힘으로 평탄하지 않은 Loss Function 위를<br>\n",
    "매끈하게 지나갈 수 있도록 도와줍니다.<br>\n",
    "* $\\beta$에 따라, 관성의 크기를 결정할 수 있습니다. 관성이 클수록, 가속이 빨라지지만<br>\n",
    "변화에 대처하는 속도가 느려, 수렴이 느려질 수 있습니다. 보통 0.9정도를 이용합니다. <br>\n",
    "\n",
    "![Imgur](https://i.imgur.com/WHfLZpK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. NAG Momentum Optimizer 수식\n",
    "\n",
    "$$\n",
    "W_{nag} = W - \\beta v \\\\\n",
    "V := \\beta v +\\frac{\\partial{L}}{\\partial{W_{nag}}} \\\\\n",
    "W := W - \\alpha V\n",
    "$$\n",
    "\n",
    "![Imgur](https://i.imgur.com/xSH4hck.jpg)\n",
    "\n",
    "* NAG 모멘텀 알고리즘은 모멘텀 알고리즘의 수정 본입니다. 차이는 어느 시점의 기울기를 이용할 것인가에 따라 달라집니다. NAG 모멘텀은 momentum에 의해 이동한 상태에서의 gradient을 구한다는 데에서 차이가 있습니다.<br>\n",
    "* NAG를 이용할 경우, Momentum 방식보다 늘 좀더 효과적인 것으로 알려져 있습니다. NAG 알고리즘이 Momentum보다 훨씬 안정적으로 수렴하기 때문입니다.\n",
    "* 실제로 Tensorflow와 같은 데에서 어떤 식으로 구현되어 있는가는 [해당 글](https://stackoverflow.com/questions/50774683/how-is-nesterovs-accelerated-gradient-descent-implemented-in-tensorflow)을 찾아보면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 3. Momentum Optimizer을 시각화하기 \\]\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Beale Function Visualization\n",
    "\n",
    "* 최적화 함수(Optimizer)를 평가하기 위해, 보통 Beale Function과 같은 test function을 많이 이용합니다.\n",
    "\n",
    "$$\n",
    "f(w_1,w_2) = (1.5 - w_1 + w_1w_2)^2+(2.25-w_1+w_1w_2^2)^2+(2.625-w_1+w_1w_2^3)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beale(w1, w2):\n",
    "    return ((1.5 - w1 + w1 * w2) ** 2 + \n",
    "            (2.25 - w1 + w1 * w2 ** 2) ** 2 + \n",
    "            (2.625 - w1 + w1 * w2 ** 3) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beale_plot():\n",
    "    # Beale function을 공간으로 치환한 것을 \n",
    "    xmin, xmax, xstep = -4.5, 4.5, .2\n",
    "    ymin, ymax, ystep = -4.5, 4.5, .2\n",
    "\n",
    "    w1s = np.arange(xmin, xmax, xstep)\n",
    "    w2s = np.arange(ymin, ymax, ystep)\n",
    "\n",
    "    w1, w2 = np.meshgrid(w1s, w2s)\n",
    "    z = beale(w1, w2)\n",
    "\n",
    "    minima = np.array([3., .5])\n",
    "    minima_ = minima.reshape(-1, 1)\n",
    "    z_minima = beale(*minima)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = plt.axes(projection='3d', elev=50, azim=-50)\n",
    "\n",
    "    ax.plot(*minima_, z_minima, 'r*', markersize=10)\n",
    "    ax.plot_surface(w1, w2, z, norm=LogNorm(), rstride=1, cstride=1,\n",
    "                    edgecolor='None', alpha=0.3, cmap=plt.cm.jet)\n",
    "\n",
    "    ax.view_init(30, 10)\n",
    "    ax.set_title(\"Beale Function Visualization\")\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlabel('$z$')\n",
    "\n",
    "    ax.set_xlim((xmin, xmax))\n",
    "    ax.set_ylim((ymin, ymax))\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = generate_beale_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent Optimizer 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(learning_rate = 1e-5)\n",
    "\n",
    "start_w1, start_w2 = -2., -4.\n",
    "\n",
    "w1 = K.variable(start_w1)\n",
    "w2 = K.variable(start_w2)\n",
    "\n",
    "history = []\n",
    "for i in tqdm(range(2000)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = beale(w1, w2)\n",
    "    history.append((w1.numpy(), w2.numpy(), out.numpy()))\n",
    "    \n",
    "    variables = [w1, w2]\n",
    "    gradients = tape.gradient(out, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_xs, gd_ys, gd_zs = zip(*history)\n",
    "\n",
    "ax = generate_beale_plot()\n",
    "ax.plot(gd_xs, gd_ys, gd_zs, \n",
    "        label='Gradient Descent', \n",
    "        color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Momentum Optimizer 시각화하기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Momentum이 0.9일 때 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = 0.9\n",
    "optimizer = SGD(learning_rate = 1e-5, momentum=momentum)\n",
    "\n",
    "start_w1, start_w2 = -2., -4.\n",
    "\n",
    "w1 = K.variable(start_w1)\n",
    "w2 = K.variable(start_w2)\n",
    "\n",
    "history = []\n",
    "for i in tqdm(range(2000)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = beale(w1, w2)\n",
    "    history.append((w1.numpy(), w2.numpy(), out.numpy()))\n",
    "    \n",
    "    variables = [w1, w2]\n",
    "    gradients = tape.gradient(out, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로를 시각화하기\n",
    "gd_xs, gd_ys, gd_zs = zip(*history)\n",
    "\n",
    "ax = generate_beale_plot()\n",
    "ax.plot(gd_xs, gd_ys, gd_zs, \n",
    "        label='momentum(beta={})'.format(momentum), \n",
    "        color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Momentum에 따른 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for momentum  in [0.5,0.9,0.99]:\n",
    "    optimizer = SGD(learning_rate = 1e-5, momentum=momentum)\n",
    "\n",
    "    start_w1, start_w2 = -2., -4.\n",
    "\n",
    "    w1 = K.variable(start_w1)\n",
    "    w2 = K.variable(start_w2)\n",
    "\n",
    "    history = []\n",
    "    for i in tqdm(range(2000)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = beale(w1, w2)\n",
    "        history.append((w1.numpy(), w2.numpy(), out.numpy()))\n",
    "\n",
    "        variables = [w1, w2]\n",
    "        gradients = tape.gradient(out, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "    # 경로를 시각화하기\n",
    "    gd_xs, gd_ys, gd_zs = zip(*history)\n",
    "\n",
    "    ax = generate_beale_plot()\n",
    "    ax.plot(gd_xs, gd_ys, gd_zs, \n",
    "            label='momentum(beta={})'.format(momentum), \n",
    "            color='b')\n",
    "plt.legend()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NAG Momentum Optimizer 시각화하기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Momentum이 0.9일 때 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = 0.9\n",
    "optimizer = SGD(learning_rate = 1e-5, momentum=momentum, nesterov=True)\n",
    "\n",
    "start_w1, start_w2 = -2., -4.\n",
    "\n",
    "w1 = K.variable(start_w1)\n",
    "w2 = K.variable(start_w2)\n",
    "\n",
    "history = []\n",
    "for i in tqdm(range(2000)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = beale(w1, w2)\n",
    "    history.append((w1.numpy(), w2.numpy(), out.numpy()))\n",
    "    \n",
    "    variables = [w1, w2]\n",
    "    gradients = tape.gradient(out, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_xs, gd_ys, gd_zs = zip(*history)\n",
    "\n",
    "ax = generate_beale_plot()\n",
    "ax.plot(gd_xs, gd_ys, gd_zs,\n",
    "        label='NAG momentum(beta={})'.format(momentum), \n",
    "        color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Momentum에 따른 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for momentum  in [0.5,0.9,0.99]:\n",
    "    optimizer = SGD(learning_rate = 1e-5, momentum=momentum,\n",
    "                    nesterov=True)\n",
    "\n",
    "    start_w1, start_w2 = -2., -4.\n",
    "\n",
    "    w1 = K.variable(start_w1)\n",
    "    w2 = K.variable(start_w2)\n",
    "\n",
    "    history = []\n",
    "    for i in tqdm(range(2000)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = beale(w1, w2)\n",
    "        history.append((w1.numpy(), w2.numpy(), out.numpy()))\n",
    "\n",
    "        variables = [w1, w2]\n",
    "        gradients = tape.gradient(out, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "    # 경로를 시각화하기\n",
    "    gd_xs, gd_ys, gd_zs = zip(*history)\n",
    "\n",
    "    ax = generate_beale_plot()\n",
    "    ax.plot(gd_xs, gd_ys, gd_zs, \n",
    "            label='nag momentum(beta={})'.format(momentum), \n",
    "            color='b')\n",
    "plt.legend()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "---\n",
    "\n",
    "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
    "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/04/05\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

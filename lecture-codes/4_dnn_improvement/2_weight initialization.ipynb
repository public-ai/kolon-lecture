{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Public AI</b></i>\n",
    "\n",
    "# Weight Initialization\n",
    "\n",
    "### _Objective_\n",
    "1. 심층신경망이 깊어짐에 따라 발생하는 Gradient Vanishing과 Gradient Exploding에 대해 배워보도록 하겠습니다. <br>\n",
    "2. 가중치를 왜 랜덤으로 초기화해야 하는지에 대해 배워보도록 하겠습니다. <br>\n",
    "3. 가중치를 초기화하는 대표적인 방법인 Xavier와 He init에 대해 배워보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 1. Gradient Vanishing과 Gradient Exploding \\]\n",
    "\n",
    "----\n",
    "\n",
    "신경망은 깊어질수록, 신경망을 학습시키기가 어려워집니다. 깊어질수록 Gradient가 사라지는 Vanishing 문제 혹은 Gradient가 지나치게 커지는 Exploding 문제가 발생하기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 순전파 과정 수식 일반화하기\n",
    "![Imgur](https://i.imgur.com/ZHZHpAy.png)\n",
    "\n",
    "\n",
    "* 위의 모델을 수식으로 풀면 아래와 같습니다.<br>\n",
    "$\n",
    "a^{0} = X\\\\\n",
    "Z^1 = a^{0}\\cdot W^{[1]} + b^{[1]}\\\\\n",
    "a^1 = \\sigma(Z^1)\\\\\n",
    "Z^2 = a^{0}\\cdot W^{[2]} + b^{[2]}\\\\\n",
    "a^2 = \\sigma{Z^2}\n",
    "$\n",
    "<br>\n",
    "* 위의 반복적인 과정을 일반화해서 나타내면, 아래의 관계가 나타납니다.<br>\n",
    "$\n",
    "Z^{i+1} = a^{i}\\cdot W^{[i+1]} + b^{[i+1]}\\\\\n",
    "a^{i+1} = \\sigma(z^{i+1})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 순전파 과정에서의 Vanishing과 Exploding\n",
    "\n",
    "* 활성화 함수($\\sigma$)와 Bias($b$) 없다고 가정해보겠습니다.<br>\n",
    "* 그렇다면, 각층의 로짓값($Z$)는 아래의 관계를 가지고 있습니다.<br>\n",
    "$\n",
    "Z^{i+1} = X\\cdot W^{[1]}\\cdot W^{[2]}... \\cdot W^{[i+1]}\n",
    "$\n",
    "\n",
    "* X의 값이 아래와 같을 때, W의 값에 따라 어떻게 변화하는지 살펴보겠습니다.<br>\n",
    "$\n",
    "X = \\begin{bmatrix}\n",
    "0.8 \\\\\n",
    "0.2 \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Weight의 원소 값이 1보다 클때\n",
    "$\n",
    "W = \\begin{bmatrix}\n",
    "1.2 & 0 \\\\\n",
    "0 & 1.2 \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.8,0.2]])\n",
    "W = np.array([[1.2,0.],[0.,1.2]])\n",
    "\n",
    "Z = X\n",
    "for i in range(50):\n",
    "    Z = np.dot(Z, W)\n",
    "    if i % 10 == 0:\n",
    "        print(\"{}th value : {}\".format(i,Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 값은 모두 0과 1 사이에 위치하였는데, 계산을 반복할수록 그 값이 기하급수적으로 증가합니다.\n",
    "\n",
    "순전파 과정에서 로짓을 계산하면서 반복적으로 Weight의 값이 곱해지게 됩니다. <br>\n",
    "Weight의 크기가 1보다 크면, 점점 값이 기하급수적으로 커지게 됩니다.<br>\n",
    "이렇게 전파 과정에서 값이 폭발하는 것을 **Exploding Problem**이라 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Weight의 원소 값이 1보다 작을 때\n",
    "$\n",
    "W = \\begin{bmatrix}\n",
    "0.8 & 0 \\\\\n",
    "0 & 0.8 \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.8,0.2]])\n",
    "W = np.array([[0.8,0.],[0.,0.8]])\n",
    "\n",
    "Z = X\n",
    "for i in range(50):\n",
    "    Z = np.dot(Z, W)\n",
    "    if i % 10 == 0:\n",
    "        print(\"{}th value : {}\".format(i,Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 값은 모두 0과 1 사이에 위치하였는데, 계산을 반복할수록 로짓값은 0에 수렴합니다.<br>\n",
    "이렇게 전파 과정에서 값이 소실되는 것을 **Vanishing Problem**이라 부릅니다. <br>\n",
    "Neural Network는 Weight의 크기에 따라 값이 크게 바뀝니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## 3. 역전파 과정에서의 Vanishing과 Exploding\n",
    "\n",
    "그렇다면, 각층의 로짓값($Z^{[i+1]}$)는 계산그래프로 표현한다면 아래의 관계를 가집니다. <br>\n",
    "\n",
    "![Imgur](https://i.imgur.com/iQvVNOn.png)\n",
    "\n",
    "위의 수식에서도 전파되는 error값에는 Weight가 반복적으로 곱해지게 됩니다.<br>\n",
    "이에 따라 Backpropagation을 하는 과정중에서 Gradient는 Vanishing 혹은 Exploding 하게 됩니다.<br>\n",
    "\n",
    "이는 초기 인공 신경망의 핵심 난제 중 하나였습니다. <br>\n",
    "이를 해결하기 위해, <br>\n",
    "1. 가중치 초기화\n",
    "2. 활성화 함수\n",
    "\n",
    "에 대한 연구가 활발히 이루어졌습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 2. Weight Initialization \\]\n",
    "\n",
    "----\n",
    "\n",
    "초기 Weight을 잘 설정해줌으로써 모델이 학습할 때, Gradient Vanishing 혹은 Gradient Exploding 문제가 발생하지 않도록 합니다. 이에 대한 해결책으로 Xavier 초기화와 He 초기화가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## 1. 좋은 초기화의 조건\n",
    "\n",
    "초기화에 따라 입력값과 출력값이 어떤 식으로 바뀌는지 한번 살펴보도록 하겠습니다.<br>\n",
    "이를 확인해보기 위해 각 레이어는 512로 이루어져 있으며, 10층짜리 깊이로 쌓여진 신경망을 구성해보도록 하겠습니다. 각 층의 가중치들은 표준편차가 1로 이루어진 정규분포로 난수를 초기화하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(512,))\n",
    "\n",
    "# units : 512\n",
    "# layer의 수 : 10\n",
    "# activation : sigmoid\n",
    "# 가중치 : 표준편차가 1로 이루어진 정규분포\n",
    "\n",
    "model = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = np.random.normal(size=(512,512), scale=0.1)\n",
    "out = model.predict(ins)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.set_title(\"Input Histogram\")\n",
    "ax.hist(ins.flatten())\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.set_title(\"Output Histogram\")\n",
    "ax.hist(out.flatten())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기화가 잘못되어 있다면, 위와 같이 입력값과 출력값의 분포가 완전히 달라지게 됩니다.<br>\n",
    "위와 같은 상황에서는 대부분 출력값이 극단적인 값 0과 1에 머물러 있다는 것을 알수 있습니다.<br> \n",
    "\n",
    "우리는 이러한 상황을 피하기 위해, 값의 분포는 층을 전파하는 과정에서 어느정도 일정한 분산을 지켜주어야 한다는 것을 직관적으로 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Xavier 초기화 유도\n",
    "\n",
    "우리는 입력값과 가중치가 정규분포를 따르고, 각각 평균이 0이라고 가정해 봅시다.<Br>\n",
    "우선 분산의 정의는 아래와 같습니다.\n",
    "$$\n",
    "var(x) = E(x^2) - E(x)^2 \\\\\n",
    "var(w) = E(w^2) - E(w)^2 \\\\\n",
    "$$\n",
    "\n",
    "우리가 가정한 평균이 0이라는 가정에 의해, 기댓값은 0이 됩니다<br>\n",
    "$$\n",
    "var(x) = E(x^2) \\\\\n",
    "var(w) = E(w^2) \\\\\n",
    "$$\n",
    "\n",
    "로짓값($z$)는 가중치와 입력값의 행렬곱입니다.<br>\n",
    "\n",
    "$$\n",
    "z_i = w_i x_i \\\\\n",
    "z = \\sum_{i=1}^n w_i x_i\n",
    "$$\n",
    "\n",
    "로짓값의 분산은 아래와 같이 전개됩니다.<br>\n",
    "\n",
    "$$\n",
    "var(w_ix_i) = E(w_i^2x_i^2) - [E(w_ix_i)]^2 \\\\\n",
    "var(w_ix_i) = E(w_i^2)E(x_i^2) - [E(w_i)E(x_i)]^2 \\\\\n",
    "var(w_ix_i) = E(w_i^2)E(x_i^2)\n",
    "$$\n",
    "\n",
    "위에서 밝혀낸 $var(x)$와 $var(w)$에 의해 아래의 수식이 증명됩니다. <br>\n",
    "\n",
    "$$\n",
    "var(w_ix_i) = var(w_i)var(x_i)\n",
    "$$\n",
    "\n",
    "그렇다면 로짓값의 분산은 아래의 수식을 만족하게됩니다.\n",
    "\n",
    "$$\n",
    "var(z) = \\sum^{n}_{i=1}var(w_ix_i)\\\\\n",
    "= \\sum^{n}_{i=1}var(w_i)var(x_i)\\\\\n",
    "= nVar(w)var(x)\n",
    "$$\n",
    "\n",
    "우리가 목적으로 하는 것은 바로 $Var(z) = Var(x)$이므로, <br>\n",
    "이를 만족하는 $var(w)$는 아래와 같습니다.<br>\n",
    "$$\n",
    "var(w) = \\frac{1}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Xavier 초기화 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "num_data = 2000\n",
    "unit_num = 512\n",
    "num_layer = 20\n",
    "\n",
    "X = np.random.randn(num_data, unit_num) \n",
    "\n",
    "history = []\n",
    "a = X\n",
    "for i in range(num_layer):\n",
    "    W = # Weight를 초기화해보기\n",
    "    z = np.dot(a,W)\n",
    "    a = sigmoid(z)\n",
    "    history.append(a.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,5))\n",
    "for idx, value in enumerate(history):\n",
    "    plt.subplot(1, num_layer, idx+1)\n",
    "    plt.title(\"{}th layer\".format(idx+1))\n",
    "    plt.hist(value.flatten(),50,range=(0,1))\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순전파 과정 중에서도 층 별 출력 값의 분포가 크게 변화하지 않음을 볼 수 있습니다.<br>\n",
    "Xavier 방식은 간단한 방법으로 출력값을 비슷하게 맞추어주는 효과가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) relu에서의 Xavier 초기화\n",
    "\n",
    "앞 강의에서 우리는 sigmoid 대신 활성화 함수로 relu를 이용하였습니다.<br>\n",
    "Relu는 가장 대표적인 활성화 함수로, sigmoid와 달리 비대칭적인 형태를 가졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "num_data = 2000\n",
    "unit_num = 512\n",
    "num_layer = 20\n",
    "\n",
    "X = np.random.randn(num_data, unit_num) \n",
    "\n",
    "history = []\n",
    "a = X\n",
    "for i in range(num_layer):\n",
    "    W = # Weight를 초기화해보기\n",
    "    z = np.dot(a,W)\n",
    "    a = relu(z)\n",
    "    history.append(a.copy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,5))\n",
    "for idx, value in enumerate(history):\n",
    "    plt.subplot(1, num_layer, idx+1)\n",
    "    plt.title(\"{}th layer\".format(idx+1))\n",
    "    plt.hist(value.flatten(),50,range=(0.01,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화 함수로 relu 함수를 이용할 경우, Xavier로 하더라도 값의 분포가 계속 줄어듭니다.<br>\n",
    "이는 relu의 형태에 기인한 문제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) He 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-1,1,0.1)\n",
    "ys = relu(xs)\n",
    "\n",
    "plt.title(\"Relu Activation Function\")\n",
    "plt.plot(xs,ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu는 강제적으로 입력 값이 0이하의 값들을 0으로 바꾸어버립니다. <br>\n",
    "이렇기 때문에 값의 분산이 줄어드는 효과가 발생합니다.<br>\n",
    "이를 보정하기 위해서 Xavier의 가중치 값의 분포에 2배를 곱하는 식으로 진행됩니다.<br>\n",
    "$$\n",
    "var(w) = \\frac{2}{n}\n",
    "$$\n",
    "위의 수식을 허(He) 초기화라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "num_data = 2000\n",
    "unit_num = 512\n",
    "num_layer = 20\n",
    "\n",
    "X = np.random.randn(num_data, unit_num) \n",
    "\n",
    "history = []\n",
    "a = X\n",
    "for i in range(num_layer):\n",
    "    W = # Weight를 초기화해보기\n",
    "    z = np.dot(a,W)\n",
    "    a = relu(z)\n",
    "    history.append(a.copy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,5))\n",
    "for idx, value in enumerate(history):\n",
    "    plt.subplot(1, num_layer, idx+1)\n",
    "    plt.title(\"{}th layer\".format(idx+1))\n",
    "    plt.hist(value.flatten(),50,range=(0.01,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xavier와 달리, 값의 분포가 유지되는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 활성화 함수와 초기화 매개변수\n",
    "\n",
    "일반적으로 활성화 함수 종류에 따라 초기화는 아래의 매개변수들을 이용합니다.\n",
    "\n",
    "| 활성화 함수 | 균등분포[-r,r] | 정규분포 |\n",
    "| ---- | ---- | ---- |\n",
    "| 로지스틱 | $r=\\sqrt{\\frac{6}{n_{inputs}+n_{outputs}}}$ | $\\sigma=\\sqrt{\\frac{2}{n_{inputs}+n_{outputs}}}$ |\n",
    "| tanh | $r=4*\\sqrt{\\frac{6}{n_{inputs}+n_{outputs}}}$ | $\\sigma=4\\sqrt{\\frac{2}{n_{inputs}+n_{outputs}}}$ |\n",
    "| RELU | $r=\\sqrt{2}\\sqrt{\\frac{6}{n_{inputs}+n_{outputs}}}$ | $\\sigma=\\sqrt{2}\\sqrt{\\frac{2}{n_{inputs}+n_{outputs}}}$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 텐서플로우에서의 초기화 함수\n",
    "\n",
    "텐서플로우에서도 위의 초기화 함수를 지원해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import he_normal, glorot_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = he_normal() # he init\n",
    "glorot_init = glorot_normal() # glorot은 xavier와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = he_init(shape=(1000, 100)).numpy()\n",
    "plt.hist(values.flatten(),bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 초기화 함수들은 아래와 같이 Tensorflow 내 다양한 High-API 메소드와<br>\n",
    "연동하여 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data를 받아오는 placeholder\n",
    "x = Input(shape=(3,))\n",
    "dense = Dense(100,activation='relu', kernel_initializer=he_init, name='hidden1')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "---\n",
    "\n",
    "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
    "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/04/04\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

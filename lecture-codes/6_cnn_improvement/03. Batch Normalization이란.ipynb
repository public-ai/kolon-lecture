{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Public AI</b></i>\n",
    "<br>\n",
    "# Batch Normalization이란\n",
    "\n",
    "### _Objective_\n",
    "* Batch Normalization은 신경망과 Hyper Parameter의 상관관계를 줄여줍니다. <br>\n",
    "* Batch Normalization은 아주 깊은 신경망이더라도, 쉽게 학습할 수 있도록 도와줍니다. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 1. 신경망을 깊게 쌓았을 때의 문제, Covariate shift \\]\n",
    "\n",
    "----\n",
    "\n",
    "이론 상, 신경망은 깊게 쌓을수록 성능이 더 좋아져야 합니다. 하지만, 실제로 깊게 쌓았을 때에는 오히려 Training Error가 더 커지는 문제가 발생하기 시작합니다. <br>\n",
    "깊게 쌓을수록, 인공 신경망의 은닉층은 학습되기가 매우 까다로워 집니다. 학습이 어려워지는 가장 대표적인 이유로 Covariate Shift가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Covariate Shift란\n",
    "\n",
    "![](https://littleml.files.wordpress.com/2013/10/covariate_shift.jpg)\n",
    "\n",
    "* Covariate shift란, Train/Test Input Data의 분포가 다른 현상입니다.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 학습데이터셋과 평가 데이터셋이 달랐을 때의 문제\n",
    "\n",
    "![Imgur](https://i.imgur.com/2AKhfjL.png)\n",
    "\n",
    "예를 들어, 고양이/개를 분류하는 문제가 있다고 해봅시다.<br> \n",
    "우리는 위와 같이 데이터셋을 구성했다고 합시다. 위와 같이 데이터셋을 구성했을 경우,<br>\n",
    "모델은 어떻게 학습될까요?\n",
    "\n",
    "털의 색깔을 위주로, 개와 고양이를 분류할 것입니다. 이렇게 학습된 모델은 오른쪽과 같이 유색의 고양이를 분류할 때에는 좋지 못한 성능을 보일 것입니다.\n",
    "\n",
    "Train Data와 Test Data의 분포가 다른 문제는 흔한 사례입니다.<br>\n",
    "대표적으로 자율 주행 자동차가 잘못 판단하여 사고가 나는 이유 중 하나도, <br>\n",
    "학습할 때와 전혀 다른 실제 상황을 맞닥뜨리게 되면서 오작동이 발생한 것입니다.\n",
    "\n",
    "Covariate Shift 현상이 발생했을 경우, 우리는 다시 Test Data에 맞게 재학습하는 절차를 거쳐야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 인공신경망에서의 Covariate Shift, **Internal Covariate Shift**\n",
    "\n",
    "인공신경망의 각 층은 이전 층의 출력층의 분포를 학습하는 형태로 진행됩니다.<br>\n",
    "\n",
    "![Imgur](https://i.imgur.com/5FKS73q.png)\n",
    "\n",
    "각 층의 가중치 갱신은 동시에 이루어지게 됩니다.<br> \n",
    "각 층의 가중치 갱신 이후, 각 층의 출력 분포는 달라지게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/nEe0AAf.png)\n",
    "\n",
    "이렇게 분포가 달라졌을 때, 각 은닉층은 새로운 분포를 학습해야 합니다.<br>\n",
    "층이 깊어지면 질수록, 위의 문제는 더 심각해져, 어느 정도 이상의 깊이가 되었을 때에는<br>\n",
    "모델은 Interal Covariate Shift로 인해 학습이 어려워지게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 2. Internal Covariate Shift를 줄이는 방법, Batch Normalization \\]\n",
    "\n",
    "----\n",
    "\n",
    "Batch Normalization의 논문 이름은 `Batch Normalization: Accelerating Deep Network Training by reducing Internal Covariate Shift`입니다. 부제에서 밝히듯, Internal Covariate Shift 문제를 해결하기 위해 고안된 방법론입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch Normalization의 목적 및 효과\n",
    "\n",
    "![Imgur](https://i.imgur.com/SEfujYn.png)\n",
    "\n",
    "논문에서 밝히듯, Deep Neural Network가 학습하지 못하는 원인으로, Internal Covariate Shift 문제를 꼽았고,<br>\n",
    "이 문제를 해결하기 위해, layer inputs을 정규화하는 방식을 도입했다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Input의 Normalization\n",
    "\n",
    "우리는 앞서, 로지스틱 회귀를 학습시킬 때, Input을 정규화하는 과정을 거쳤습니다.<br>\n",
    "보다 빠르게 학습시켜주고, 각 Input의 영향력을 균등하게 만들어주어, <br>\n",
    "보다 값이 안정적으로 계산되도록 하였습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/HXmoSEa.png)\n",
    "\n",
    "Input 값의 정규화는 아래와 같이 진행됩니다.<br>\n",
    "\n",
    "$\n",
    "\\mu = \\frac{1}{m}\\sum_i x^{(i)} \\\\\n",
    "\\sigma^2 = \\frac{1}{m}\\sum_i (x^{(i)})^2 \\\\\n",
    "x_{norm} = \\frac{x-\\mu}{\\sigma}\n",
    "$\n",
    "\n",
    "Batch Normalization의 기본 아이디어는 각 층마다의 입력 값을 Mini-Batch 단위로<br>\n",
    "정규화하는 것에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 정규화의 효과\n",
    "\n",
    "![Imgur](https://i.imgur.com/ufU3RxH.png)\n",
    "\n",
    "각 층을 정규화해줌으로써, 인공 신경망의 Hyper Parameter에 대한 민감도를 줄였습니다.<br>\n",
    "인공 신경망은 학습률, 초기화 등의 Hyper Parameter에 의해 성능이 크게 좌지우지됩니다.<br>\n",
    "하지만 Batch Normalization은 각층의 값을 표준화해줌으로써, <br>\n",
    "hyper Parameter에 의한 영향력을 크게 줄였습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/DzJzZmN.png)\n",
    "\n",
    "이로 인해, 위와 같이 배치 노말을 적용했을 때, 훨씬 더 빠른 속도로 수렴하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Normalization의 알고리즘\n",
    "\n",
    "Batch Normalization을 올바르게 구현하기 위해서는, 해당 알고리즘을 올바르게 파악하고 있어야 합니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 미니 배치 단위로 정규화하기\n",
    "\n",
    "인공 신경망 내 은닉층의 입력값이 아래와 같이 D차원으로 구성되어 있다고 가정합니다.<br>\n",
    "\n",
    "$$\n",
    "X = (x^{(1)},x^{(2)}, ..., x^{(d)})\n",
    "$$\n",
    "\n",
    "올바르게 정규화를 하기 위해서는 우리는 모든 데이터셋으로 계산해, 평균과 분산을 구해 아래와 같이 정규화를 해야 합니다.<br>\n",
    "\n",
    "$$\n",
    "\\hat X^{(k)} = \\frac{x^{(k)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}\n",
    "$$\n",
    "\n",
    "하지만 전체 데이터셋으로 계산하게 되면, 매 스텝마다 각 층 별로 변화하는 평균과 분산을 <br>\n",
    "추적해야 하기 때문에 너무 많은 계산 비용이 발생하게 됩니다. <br>\n",
    "\n",
    "우리는 전체 데이터셋이 아닌 **미니 배치** 별로 평균과 분산을 구함으로써, 약간의 오차가 발생하는 대신<br> 훨씬 더 빠르게 정규화를 구현할 수 있습니다.<br>\n",
    "\n",
    "각 Mini-Batch가 $\\{x_{1},...,x_{m}\\}$으로 주어졌다면, 우리는 평균과 표준편차를 아래와 같이 구할 수 있습니다.<br>\n",
    "\n",
    "$\n",
    "\\mu_{B}^{(k)} = \\frac{1}{m}\\sum_{i=1}^{m}x_i^{(k)} \\\\\n",
    "(\\sigma_{B}^{(k)})^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_i^{(k)}-\\mu_B^{(k)})^2 \\\\\n",
    "\\hat x^{(k)} = \\frac{x_i - \\mu_{B}}{\\sqrt{\\sigma_{B}^2+\\epsilon}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Batch Normalizing Transform\n",
    "\n",
    "위와 같이 단순히 정규화를 한다면, 모든 층에서의 입력값은 $N ~ (0, 1)$ 즉, 평균은 0이고 분산은 1인 정규분포 형태를 띄게 됩니다. 이렇게 될 경우, 각 층마다 적절한 수준의 평균과 분산과는 다를 수 있기 때문에 학습에 지장을 초래할 수 있습니다. 논문에서는 아래와 같이 설명하고 있습니다.<br>\n",
    "\n",
    "![Imgur](https://i.imgur.com/YIEGVA3.png)\n",
    "\n",
    "수학적으로 엄밀히 증명되진 않았습니다만, 직관적으로 보았을 때 저차원에서의 입력 분포와 고차원에서의 입력 분포는 동일하지 않을거라 쉽게 추측할 수 있습니다. <br>\n",
    "\n",
    "Batch Normalization에서는 각 층마다의 적절한 평균과 분산을 보정해주기 위해, 2개의 Weight, Scale Factor($\\gamma^{(k)}$)와 Shift Factor($\\beta^{(k)}$)를 별도로 구성합니다.<br>\n",
    "$y^{(k)} = \\gamma^{(k)}\\hat x ^{(k)} + \\beta^{(k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale Factor와 Shift Factor은 둘 다 Training 과정에서 학습되는 파라미터입니다.<br>\n",
    "역전파 과정은 아래와 같이 진행됩니다.<br>\n",
    "\n",
    "![Imgur](https://i.imgur.com/wc7irYK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Batch Normalization 과정 요약하기\n",
    "\n",
    "배치 노말 과정은 로짓 계산 후, 활성화 함수를 거치기 전에 포함됩니다. \n",
    "\n",
    "![](https://i0.wp.com/mohammadpz.github.io/posts/2015_02_01_IFT6266_Cats_vs_Dogs/img/bn.png?zoom=2)\n",
    "\n",
    "$\n",
    "z = X\\cdot W + b, \\cdots \\mbox{ (1) 로짓 계산}\\\\\n",
    "y = BN(z), \\cdots \\mbox{ (2) 배치노말 적용 }\\\\\n",
    "a = \\sigma(y), \\cdots \\mbox{ (3) 활성화 함수 적용 }\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 하나 알아두어야 하는 점 중 하나는 <br>\n",
    "배치노말에서 정규화를 거치게 될 때, 로짓 계산에서 들어가는 bias는 사실상 무시하게 됩니다.<br>\n",
    "\n",
    "그렇기 때문에 Batch Normalization을 적용할 경우, 이전 Layer에서의 bias는 생략하는 것이 관례입니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "z = X\\cdot W, \\cdots \\mbox{ (1) 로짓 계산}\\\\\n",
    "y = BN(z), \\cdots \\mbox{ (2) 배치노말 적용 }\\\\\n",
    "a = \\sigma(y), \\cdots \\mbox{ (3) 활성화 함수 적용 }\\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "---\n",
    "\n",
    "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
    "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/05/07\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

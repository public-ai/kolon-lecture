{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Public AI</b></i>\n",
    "<br>\n",
    "# ResNet의 여러 직관들\n",
    "\n",
    "### _Objective_\n",
    "\n",
    "* ResNet은 2015년 이미지넷 대회에서 압도적인 1위를 기록한 대회로, 사람의 에러율보다 낮아지기 시작한 첫 모델입니다. <br>\n",
    "* ResNet의 핵심인 Skip Connection & Residual Block에 대해 배워보도록 하겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ 깊어진 모델의 성능이 떨어지는 이유, Degradation Problem \\]\n",
    "---\n",
    "\n",
    "이론적으로 모델이 깊어지면 질수록, Parameter의 수가 늘어나고 비선형성이 증대되기 때문에 성능이 좋아져야 합니다. 하지만 무작정 깊게 하면 오히려 Training Loss가 높아지는 Degradation 문제가 발생합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Degradation 문제란? \n",
    "\n",
    "\n",
    "Batch Normalization와 여러 초기화 기법들이 개발되면서, 깊은 신경망을 학습시킬 때 발생하는 Vanishing & Exploding Gradient 문제는 어느정도 해결되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 20-layer의 Network VS 56-layer의 Network\n",
    "\n",
    "<img src=\"https://i.imgur.com/YoiOOkK.png\" width=\"500\">\n",
    "\n",
    "ResNet팀이 가진 첫번째 의문은 바로 위의 문제였습니다.<br>\n",
    "> 왜 깊어졌을 때, 오히려 성능이 떨어질까? \n",
    "\n",
    "Test의 성능이 떨어지는 문제는 Overfitting이지만, Training의 성능이 떨어지는 문제는 이론과는 다른 형태의 문제였습니다.\n",
    "\n",
    "Batch Normalization이 개발되기 전까지는, 깊은 망에서 Gradient가 적절히 전파되지 못하는 Vanishing & Exploding Gradient 문제를 주요 원인이라 파악하였습니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/yNyy2u2.png\" width=\"500\">\n",
    "\n",
    "하지만 Batch normalization이 생김으로써, 실제로 Gradient의 값을 따져보았을 때, 더이상 Vanishing & Exploding Gradient의 문제가 아니게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Degradation 문제란? \n",
    "\n",
    "<img src = \"https://i.imgur.com/aiCujMR.png\" width=\"500\">\n",
    "\n",
    "Degradation 문제는 Overfitting 문제와 다르게, Training의 성능 지표가 깊이에 따라 떨어지는 현상을 의미합니다. 이 문제는 Hyper-Parameter로서 Layer의 깊이가 매우 중요한 요소가 된다는 것을 의미하게 됩니다.<br>\n",
    "Hyper-Parameter가 복잡해 질수록, 우리는 모델을 최적화하기 어려워집니다. Degradation 문제가 없었더라면, 우리의 모델 설계는 매우 간단해 집니다. 정확도를 높이기 위해서는 좀 더 깊게 쌓기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> 이제 여기서부터 ResNet 팀은 가정을 해보기 시작합니다. \n",
    "\n",
    "\n",
    "<img src = \"https://i.imgur.com/Nz3C4Zd.png\" width=\"500\">\n",
    "\n",
    "----\n",
    "\n",
    "즉 위의 문장을 도식화해보면 아래와 같습니다.\n",
    "\n",
    "<img src = \"https://i.imgur.com/6mCrRHl.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 도식적으로 그려보았을 때, Degradation의 문제를 해결하는 방법은 매우 간단합니다.<br>\n",
    "바로 붉은 색의 추가된 Layer 부분이 항등 함수(Identity Function), 즉 \n",
    "$$\n",
    "Y' = F(Y) \n",
    "$$\n",
    "을 만족하는 함수가 되면 됩니다. 항등함수로 된다면, Swallower Network의 출력과 Deeper Network의 출력이 같아지기 때문에 성능이 동일하게 나옵니다.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Degradation의 해결책, Residual Learning\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Residual Mapping\n",
    "\n",
    "ResNet 팀에서는 새로운 연결 방식을 제안합니다.<br>\n",
    "![Imgur](https://i.imgur.com/pcr2TwD.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매우 단순한 아이디어로, 입력으로 들어온 값을 출력층에 더해주는 것입니다.<br>\n",
    "\n",
    "$$\n",
    "H(x) = F(x) + x\n",
    "$$\n",
    "이 모델에서의 F(x)가 실제로 학습하게 되는 것은 <br>\n",
    "$$\n",
    "F(x) = H(x) - x\n",
    "$$\n",
    "즉, 입력과 출력의 차이인 잔차(Residual)을 학습하게 됩니다. 이러한 연결이 있는 Block을 Residual Block이라 부르고, 각 Residual Block은 이전 입력에서 좀 더 개선할 요소들만 더해가는 식으로 학습하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Residual Network \n",
    "\n",
    "VGG Network의 각 층에 Batch Normalization이 추가된 모델을 Plain Network라고 합니다.<br>\n",
    "Residual Network는 Plain Network에 Skip Connection을 추가하면서 구성한 형태를 의미합니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/ORPcsh8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Residual의 효과 \n",
    "\n",
    "![Imgur](https://i.imgur.com/O3yFunn.png)\n",
    "\n",
    "plain Network의 깊이에 따른 성능 변화와, Residual Network의 깊이에 따른 성능 변화입니다.<br> 여기서 알수 있듯, Residual Network는 Degradation Network의 문제를 훌륭히 해결했을 뿐만 아니라, 성능 개선도 크게 이루어낸 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "---\n",
    "\n",
    "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
    "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/05/14\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

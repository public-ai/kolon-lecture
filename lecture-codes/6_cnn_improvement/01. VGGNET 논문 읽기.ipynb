{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Public AI</b></i>\n",
    "<br>\n",
    "# VGG Net 논문 읽기\n",
    "\n",
    "### _Objective_\n",
    "* VGGNet의 논문을 읽고 구현하면서, VGGNet 모델을 이해해 보겠습니다. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ Paper Summary \\] \n",
    "---\n",
    "\n",
    "VGG Network는 이미지 분류 대회인 ILSVRC 대회에서 2등을 기록한 모델입니다. 구조적으로 매우 간단하면서도, 높은 성능을 기록해, 이후 많은 모델들이 이를 기반으로 구성되었습니다. <br>\n",
    "\n",
    "![Imgur](https://i.imgur.com/l5DgFQo.png)\n",
    "\n",
    "Reference : [Very Deep Convolutional Networks For Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN 네트워크를 설계할때의 어려움\n",
    "\n",
    "Convolution Layer에는 개발자, 연구자가 설정해주어야 하는 다양한 설정값들이 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "Conv2D?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특히 중요한 설정값(Hyper-Parameter)들로는 아래와 같이 있습니다.\n",
    "\n",
    "* `filters`: Integer, the dimensionality of the output space\n",
    "* `kernel_size`: An integer or tuple/list of 2 integers, specifying the\n",
    "    height and width of the 2D convolution window.\n",
    "* `strides`: An integer or tuple/list of 2 integers,\n",
    "    specifying the strides of the convolution along the height and width.\n",
    "* `padding`: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
    "* `activation`: Activation function to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filters의 갯수를 얼마로 하냐, kernel_size를 얼마로 하냐 등에 따라 CNN 모델의 성능은 천차만별로 나타나게 됩니다.<br>\n",
    "뿐만 아니라 Convolution Layer를 병렬로 연결할 수도 있고, 직렬로 연결할 수 있고, Pooling Layer를 어떻게 연결하냐도 우리는 고민해야하는 문제입니다\n",
    "그렇기 때문에 개발자와 연구자들은 수없이 많은 실험을 통해 성공하는 모델을 도출해야 했습니다.\n",
    "\n",
    "2014년, VGG Network가 만들어질 때 쯤 Google에서 만든 GoogleNet이라는 네트워크가 있는데, 그 구조는 아래와 같습니다.\n",
    "\n",
    "![](https://miro.medium.com/max/5176/1*ZFPOSAted10TPd3hBQU8iQ.png)\n",
    "\n",
    "멀리서 보기만 하더라도 매우 복잡한 구조를 가지고 있습니다. 이렇게 만들어진 네트워크는 성능이 우수했지만, 다른 개발자들이 이용하기에는 너무 모델이 복잡하고, 튜닝이 어렵다는 단점이 존재했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VGG 네트워크의 구조 \n",
    "\n",
    "VGG 네트워크는 이와 다르게, 매우 단순하게 만들어진 모델입니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/oANjdNh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG Network는 Convolution Layer와 Pooling Layer을 반복적으로 쌓고, Convolutino Layer의 주요 Hyper-Parameter들을 아래와 같이 통일하였습니다.\n",
    "\n",
    "* `kernel_size`: (3, 3)\n",
    "* `strides`: (1, 1)\n",
    "* `padding`: \"same\"\n",
    "* `activation`: RELU\n",
    "\n",
    "그리고 filters는 Pooling Layer을 지날 때마다 2배씩 증가하는 식으로 구성하였습니다. 이런 단순한 설계로도 위 GoogleNe과 거의 비슷한 수준의 성능을 내서, 이후 많은 연구자와 개발자들은 VGG Network 구조와 유사한 방식으로 모델을 설계하기 시작했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ Paper Implementation \\]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Input 구성하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/HNgH2B6.png)\n",
    "\n",
    "* Input의 크기는 (224,224,3)입니다.<br>\n",
    "* 학습시킨 데이터셋의 라벨 수는 1,000개입니다.<br>\n",
    "* 전처리로서는 image의 mean value를 빼주는 식으로 진행되었습니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224,3)\n",
    "num_classes = 1000\n",
    "\n",
    "inputs = Input(input_shape, name='images')\n",
    "\n",
    "rgb_mean = np.array([123.68, 116.779, 103.939], np.float32)\n",
    "preprocessed = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Inference Network 구성하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/16z2FhL.png)\n",
    "\n",
    "위의 네트워크 중 D 타입으로 구현해보도록 하겠습니다. Paper 마다 자신의 네트워크를 보다 잘 설명하기 위한 표기방법이 있습니다. VGGNet에서는 `conv<receptive field size>-<number of channels>`방식으로 크기를 설명해주었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) VGG Net의 세부 사항들 \n",
    "\n",
    "Convolution Layer의 구성은 아래 논문에 서술 되어 있습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/EZWrJkY.png)\n",
    "\n",
    "* stride : 1\n",
    "* padding : SAME\n",
    "* Activation Function : RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 첫번째 VGG Block 구성하기\n",
    "\n",
    "VGG Net은 단순히 (3,3) 크기의 Convolution Layer가 Maxpool과 함께 적층되어 있는 구조입니다.<br> \n",
    "이전의 Network들은 (5,5), (11,11) 크기의 Filter들을 이용했는데, <br>\n",
    "VGG Net은 (3,3)만으로 충분히 깊게 쌓는 방식으로 모델을 구성하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 1\n",
    "conv = #\n",
    "conv = #\n",
    "pool = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 많은 모델에는 Regularization을 통해 모델의 학습이 안정화합니다. <br>\n",
    "해당 모델은 weight decay를 추가시켜 모델의 학습을 도왔습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/xVlbHUd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 1\n",
    "conv = #\n",
    "conv = #\n",
    "pool = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 2~5번째 VGG Block 구성하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/uTGrtng.png)\n",
    "\n",
    "VGG 연구자들은 filter size가 (7,7)인 Convolution Layer 한 층을 구성하는 것보다<br>\n",
    "filter size가 (3,3)인 Convolution Layer 3 층을 구성하는 것이<br>\n",
    "파라미터 수를 줄이면서, Non-Linearlity를 확보할 수 있었다고 말합니다.\n",
    "\n",
    "이는 현대 많은 Neural Network가 큰 Filter를 쓰지 않고, (3,3) Filter만으로 깊게 쌓도록 <br>\n",
    "하는데 결정적인 연구가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 2\n",
    "conv = #\n",
    "conv = #\n",
    "pool = #\n",
    "\n",
    "### Block 3\n",
    "conv = #\n",
    "conv = #\n",
    "conv = #\n",
    "pool = #\n",
    "    \n",
    "### Block 4    \n",
    "conv = #\n",
    "conv = #\n",
    "conv = #\n",
    "pool = #\n",
    "    \n",
    "### Block 5\n",
    "conv = #\n",
    "conv = #\n",
    "conv = #\n",
    "pool = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Receptive Field 계산하기\n",
    "\n",
    "Convolution Layer을 적층하였을 때, 가장 먼저 체크해야 하는 것 중 하나가<br>\n",
    "Convolution Layer의 Receptive Field 크기가 영상을 파악하기에 충분한 크기 수준으로<br>\n",
    "확보되었는가입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layer | Filter size | Stride | Receptive Field |\n",
    "| ----  | ------ | ----- | ------ |\n",
    "| block1-conv1 | (3,3) | (1,1) | (3,3) |\n",
    "| block1-conv2 | (3,3) | (1,1) | (5,5) |\n",
    "| maxpool1 | (2,2) | (2,2) | (6,6) |\n",
    "| block2-conv1 | (3,3) | (1,1) | (10,10) |\n",
    "| block2-conv2 | (3,3) | (1,1) | (14,14) |\n",
    "| maxpool2 | (2,2) | (2,2) | (16,16) |\n",
    "| block3-conv1 | (3,3) | (1,1) | (24,24) |\n",
    "| block3-conv2 | (3,3) | (1,1) | (32,32) |\n",
    "| block3-conv3 | (3,3) | (1,1) | (40,40) |\n",
    "| maxpool3 | (2,2) | (2,2) | (44,44) |\n",
    "| block4-conv1 | (3,3) | (1,1) | (60,60) |\n",
    "| block4-conv2 | (3,3) | (1,1) | (76,76) |\n",
    "| block4-conv3 | (3,3) | (1,1) | (92,92) |\n",
    "| maxpool4 | (2,2) | (2,2) | (100,100) |\n",
    "| block5-conv1 | (3,3) | (1,1) | (132,132) |\n",
    "| block5-conv2 | (3,3) | (1,1) | (164,164) |\n",
    "| block5-conv3 | (3,3) | (1,1) | (196,196) |\n",
    "| maxpool5 | (2,2) | (2,2) | (212,212) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 Max pool layer의 1 pixel 당 (212,212)만큼의 입력값 공간을 바라보고 있습니다.<br>\n",
    "영상의 크기가 (224,224)이므로 receptive Field의 크기가 충분하다고 파악할 수 있습니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Fully Connected Layer 구성하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/jTYH9Ze.png)\n",
    "\n",
    "논문에는 세부 구현 내용들이 곳곳히 숨겨져 있습니다.<br>\n",
    "Fully Connected Layer에는 Dropout으로 Regularization을 해주어야 합니다.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block 6\n",
    "fc = #\n",
    "fc = #\n",
    "\n",
    "dropout = #\n",
    "fc = #\n",
    "\n",
    "dropout = #\n",
    "pred = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = Model(inputs, pred)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. 학습에 관련된 부분들 구성하기\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/C9RtVJA.png)\n",
    "\n",
    "multinomial Logistic은 다른 말로 Softmax Function입니다. Softmax Function에 대응되는 Loss Function으로는 주로 Categorical CrossEntropy를 이용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Loss Function 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import #\n",
    "\n",
    "loss = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 이전에 추가한 L2 Regularization들은 아래와 같이 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv1-1_2/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv1-2_2/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv2-2/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv3-1/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv3-2/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv3-3/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv4-1/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv4-2/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv4-3/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv5-1/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv5-2/kernel/Regularizer_3/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv5-3/kernel/Regularizer_3/add:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l2 regularization\n",
    "model.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Optimizer 구성하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/dVyX1fX.png)\n",
    "\n",
    "VGG 모델을 학습시킬 때에는 Optimizer로서 Momentum Optimizer을 이용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "---\n",
    "\n",
    "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
    "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/05/07\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

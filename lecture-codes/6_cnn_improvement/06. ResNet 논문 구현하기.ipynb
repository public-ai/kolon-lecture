{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>Public AI</b></i>\n",
    "<br>\n",
    "# ResNet 논문 구현하기\n",
    "\n",
    "### _Objective_\n",
    "\n",
    "* Residual Network의 논문을 읽고 구현하면서, ResNet의 모델을 이해해 보겠습니다. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# \\[ Paper Implementation \\]\n",
    "---\n",
    "\n",
    "\n",
    "VGG Network처럼 매우 간단한 형태로, Inception Network에 비해 짜기가 매우 간단합니다.\n",
    "\n",
    "Reference : [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Placeholder 구성하기\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/1MyBu1b.png\" width=\"500\">\n",
    "\n",
    "* VGG Network와 마찬가지로 이미지의 크기는 (224,224,3)이며, 전처리로서는 Image의 Mean Value를 빼주는 방식으로 진행되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "input_shape = (224,224,3)\n",
    "\n",
    "inputs = Input(input_shape, name='images')\n",
    "\n",
    "rgb_mean = np.array([123.68, 116.779, 103.939], np.float32)\n",
    "preprocessed = (inputs - rgb_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Inference Network 구성하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/ajlup9L.png)\n",
    "* ResNet에서 핵심은 실선과 점선으로 이루어진 Residual Block입니다.<br>\n",
    "* 실선으로 이루어진 부분은 Input과 Output의 shape가 같아서, 바로 더해줄 수 있는 Block을 의미하고<br>\n",
    "점선으로 이루어진 부분은 Input과 Output의 Shape가 달라서, 바로 더해줄 수 없고 stride와 Projection을 통해 Shape을 동일하게 해주어야 합니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/KRR62oi.png)\n",
    "\n",
    "ResNet-34을 구성해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) conv2_x까지 구성하기\n",
    "\n",
    "그림에는 나타나 있지 않지만, ResNet의 모든 Convolution Layer 다음에는 Batch Normalization Layer이 뒤따라 옵니다.\n",
    "\n",
    "<img src=\"https://i.imgur.com/g3zZGs7.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv2D(64, (7,7), strides=(2,2),padding='SAME')(preprocessed)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = ReLU()(bn)\n",
    "out = MaxPooling2D((3,3),(2,2),padding='SAME')(act)\n",
    "\n",
    "# conv2_1\n",
    "conv = Conv2D(64, (3,3), padding='SAME')(out)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = ReLU()(bn)\n",
    "conv = Conv2D(64, (3,3), padding='SAME')(act)\n",
    "bn = BatchNormalization()(conv)\n",
    "added = out + bn\n",
    "out = ReLU()(added)\n",
    "\n",
    "# conv2_2\n",
    "conv = Conv2D(64, (3,3), padding='SAME')(out)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = ReLU()(bn)\n",
    "conv = Conv2D(64, (3,3), padding='SAME')(act)\n",
    "bn = BatchNormalization()(conv)\n",
    "added = out + bn\n",
    "out = ReLU()(added)\n",
    "\n",
    "# conv2_3\n",
    "conv = Conv2D(64, (3,3), padding='SAME')(out)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = ReLU()(bn)\n",
    "conv = Conv2D(64, (3,3), padding='SAME')(act)\n",
    "bn = BatchNormalization()(conv)\n",
    "added = out + bn\n",
    "out = ReLU()(added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Conv3_x 구성하기\n",
    "<img src=\"https://i.imgur.com/ctUSU6L.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv2 block에서 conv3 block으로 넘어갈 때, Input과 output의 크기가 달라집니다.<br>\n",
    "1. Feature Map의 갯수가 커지는 것 : 1x1 Conv로 차원을 늘리는 것\n",
    "2. Feature Map의 size가 줄어드는 것 : Stride를 통해 줄임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = tf.layers.Conv2D(128,(1,1),strides=(2,2))(out)\n",
    "\n",
    "conv = Conv2D(128, (3,3), strides=(2,2), padding='SAME')(out)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = ReLU()(bn)\n",
    "conv = Conv2D(128, (3,3), padding='SAME')(act)\n",
    "bn = BatchNormalization()(conv)\n",
    "added = projection + bn\n",
    "out = ReLU()(added)\n",
    "\n",
    "conv = Conv2D(128, (3,3), padding='SAME')(out)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = ReLU()(bn)\n",
    "conv = Conv2D(128, (3,3), padding='SAME')(act)\n",
    "bn = BatchNormalization()(conv)\n",
    "added = out + bn\n",
    "out = ReLU()(added)\n",
    "\n",
    "conv = Conv2D(128, (3,3), padding='SAME')(out)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = ReLU()(bn)\n",
    "conv = Conv2D(128, (3,3), padding='SAME')(act)\n",
    "bn = BatchNormalization()(conv)\n",
    "added = out + bn\n",
    "out = ReLU()(added)\n",
    "\n",
    "conv = Conv2D(128, (3,3), padding='SAME')(out)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = ReLU()(bn)\n",
    "conv = Conv2D(128, (3,3), padding='SAME')(act)\n",
    "bn = BatchNormalization()(conv)\n",
    "added = out + bn\n",
    "out = ReLU()(added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크게 두 가지 유형의 Residual이 존재한다는 것을 알 수 있습니다.<br>\n",
    "Input과 Output의 크기가 동일한 residual block(실선으로 표시)과 <br>\n",
    "Input과 Output의 크기가 다른 residual block(점선으로 표시)로 나뉘어집니다.\n",
    "\n",
    "우리는 이렇게 동일한 패턴으로 되어있을 경우, 메소드로 작성하는 것이 보다 간결하고<br>\n",
    "안전하게 코드를 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(input_layer, filters, strides=(1,1)):\n",
    "    if input_layer.shape.as_list()[-1] != filters:\n",
    "        # input_layer의 필터 갯수와 filters가 다르면, projection layer을 거침\n",
    "        projection = Conv2D(\n",
    "            filters, (1,1), strides=strides, padding='SAME')(input_layer)\n",
    "    else:\n",
    "        # 동일하면 바로 이어줌\n",
    "        projection = input_layer\n",
    "        \n",
    "    conv = Conv2D(filters, (3,3), strides, padding='SAME')(input_layer)\n",
    "    bn = BatchNormalization()(conv)\n",
    "    act = ReLU()(bn)\n",
    "    conv = Conv2D(filters, (3,3), padding='SAME')(act)\n",
    "    bn = BatchNormalization()(conv)\n",
    "    added = projection + bn\n",
    "    out = tf.nn.relu(added)       \n",
    "\n",
    "    return out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 전체 Residual Block 구성하기\n",
    "\n",
    "위에 구성한 메소드를 이용해 Residual Block을 conv5_x까지 마저 구하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224,3)\n",
    "num_classes = 1000\n",
    "\n",
    "inputs = Input(input_shape, name='images')\n",
    "\n",
    "rgb_mean = np.array([123.68, 116.779, 103.939], np.float32)\n",
    "preprocessed = (inputs - rgb_mean)\n",
    "\n",
    "conv = Conv2D(64, (7,7), strides=(2,2),padding='SAME')(preprocessed)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = ReLU()(bn)\n",
    "out = MaxPooling2D((3,3),(2,2),padding='SAME')(act)\n",
    "\n",
    "res2_1 = residual_block(out, 64)\n",
    "res2_2 = residual_block(res2_1, 64)    \n",
    "res2_3 = residual_block(res2_2, 64) \n",
    "\n",
    "res3_1 = residual_block(res2_3, 128, strides=(2,2))\n",
    "res3_2 = residual_block(res3_1, 128)    \n",
    "res3_3 = residual_block(res3_2, 128)    \n",
    "res3_4 = residual_block(res3_3, 128)    \n",
    "\n",
    "res4_1 = residual_block(res3_4, 256, strides=(2,2))\n",
    "res4_2 = residual_block(res4_1, 256)    \n",
    "res4_3 = residual_block(res4_2, 256)    \n",
    "res4_4 = residual_block(res4_3, 256)    \n",
    "res4_5 = residual_block(res4_4, 256)    \n",
    "res4_6 = residual_block(res4_5, 256)\n",
    "\n",
    "res5_1 = residual_block(res4_6, 512, strides=(2,2))\n",
    "res5_2 = residual_block(res5_1, 512)    \n",
    "res5_3 = residual_block(res5_2, 512)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Global Average Pooling Layer 구성하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/kbVvJCU.png)\n",
    "\n",
    "ResNet의 마지막 층에서는 Global Average Pooling Layer가 붙습니다.<br>\n",
    "Global Average Pooling Layer는 각 필터 층 별로 평균값을 산출하는 것을 의미합니다.<br>\n",
    "\n",
    "Global Average Pooling Layer가 붙을 경우, Input Image의 크기에 무관하게, 분류기에 넣을 수 있게 됩니다.<Br>\n",
    "ResNet 이후 많은 모델들은 Global Average Pooling Layer를 Classification 모델 마지막에 넣음으로써 이미지의 크기에 무관한 모델로 구현하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = GlobalAveragePooling2D()(res5_3)\n",
    "pred = Dense(num_classes, activation='softmax')(gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs, pred)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "---\n",
    "\n",
    "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
    "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/05/14\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
